# %% markdown
# # Mod 4 Project - Starter Notebook
#
# This notebook has been provided to you so that you can make use of the following starter code to help with the trickier parts of preprocessing the Zillow dataset.
#
# The notebook contains a rough outline the general order you'll likely want to take in this project. You'll notice that most of the areas are left blank. This is so that it's more obvious exactly when you should make use of the starter code provided for preprocessing.
#
# **_NOTE:_** The number of empty cells are not meant to infer how much or how little code should be involved in any given step--we've just provided a few for your convenience. Add, delete, and change things around in this notebook as needed!
#
# # Some Notes Before Starting
#
# This project will be one of the more challenging projects you complete in this program. This is because working with Time Series data is a bit different than working with regular datasets. In order to make this a bit less frustrating and help you understand what you need to do (and when you need to do it), we'll quickly review the dataset formats that you'll encounter in this project.
#
# ## Wide Format vs Long Format
#
# If you take a look at the format of the data in `zillow_data.csv`, you'll notice that the actual Time Series values are stored as separate columns. Here's a sample:
#
# <img src='~/../images/df_head.png'>
#
# You'll notice that the first seven columns look like any other dataset you're used to working with. However, column 8 refers to the median housing sales values for April 1996, column 9 for May 1996, and so on. This This is called **_Wide Format_**, and it makes the dataframe intuitive and easy to read. However, there are problems with this format when it comes to actually learning from the data, because the data only makes sense if you know the name of the column that the data can be found it. Since column names are metadata, our algorithms will miss out on what dates each value is for. This means that before we pass this data to our ARIMA model, we'll need to reshape our dataset to **_Long Format_**. Reshaped into long format, the dataframe above would now look like:
#
# <img src='~/../images/melted1.png'>
#
# There are now many more rows in this dataset--one for each unique time and zipcode combination in the data! Once our dataset is in this format, we'll be able to train an ARIMA model on it. The method used to convert from Wide to Long is `pd.melt()`, and it is common to refer to our dataset as 'melted' after the transition to denote that it is in long format.
#
# # Helper Functions Provided
#
# Melting a dataset can be tricky if you've never done it before, so you'll see that we have provided a sample function, `melt_data()`, to help you with this step below. Also provided is:
#
# * `get_datetimes()`, a function to deal with converting the column values for datetimes as a pandas series of datetime objects
# * Some good parameters for matplotlib to help make your visualizations more readable.
#
# Good luck!
#
#
# # Step 1: Load the Data/Filtering for Chosen Zipcodes
# %% markdown
# -  what are the top 5 best zipcodes for us to invest in?

# %%
import pandas as pd
import numpy as np
import statsmodels as sm
import matplotlib.pyplot as plt
import seaborn as sns
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
from statsmodels.tsa.arima_model import ARIMA
from sklearn.metrics import mean_squared_error

# %%
# load the Data
df = pd.read_csv('zillow_data.csv')
# %%
df.head()
# %%
df.info()
# %%
# df_0.describe
len(df.RegionName.unique())
# df_0.describe()

# %%
# drop columns
df_0 = df.drop(['SizeRank', 'State','RegionID','City','Metro','CountyName','RegionID'] , axis = 1)

# %%
df_0.head()
# %%
# check for null values
# df_0.isnull().sum()
# %%
df_0.columns
# %%
# replace the null values
df_1 = df_0.fillna(method='ffill')
# %%
# df_1.isnull().sum()
# %% markdown
# # Step 2: Data Preprocessing
# %%
def get_datetimes(df):
    return pd.to_datetime(df.columns.values[1:], format='%Y-%m')
# %%
# df_1 is our index in datetime obj we want to make a new dataframe

df_index_1 = get_datetimes(df_1)
df_index_1
# %%
# we transpose df_1
df_transpose = df_1.transpose()
df_transpose.drop('RegionName', inplace = True) # we drop

# %%
df_3 = df_1.loc[:,'RegionName']
df_4 = pd.DataFrame(df_3)
df_transpose.index = df_index_1
# %%
df_4.shape
# %%

# %%
# merging the transpose df with column RegionName
df_transpose.columns = df_4.RegionName
# %%
df_transpose.head()
# %%
# chicago code time series
df_5 = df_transpose.loc[:, 60657]
# %%
df_resample = df_transpose.resample("A")
df_yearly_mean = df_resample.mean()
df_yearly_mean

# %%
# df_try1 = df_yearly_mean.loc[:, 79936]
# %%
last_year = df_yearly_mean.iloc[1]
# %%
first_year = df_yearly_mean.iloc[-1]
# %%
difference = last_year - first_year
# %%
type(difference)
# %%
# to filter the zip codes
choosen = difference.nlargest(5)
# %%
choosen
# %%
df_5 = df_yearly_mean.loc[:, [10021,24054,29691,67877, 28039 ]]
# %% markdown
# # Step 3: Reshape from Wide to Long Format
# %%
df_5[df_]
# %%

# %%
def melt_data(df):
    melted = pd.melt(df, id_vars=['RegionName', 'City', 'State', 'Metro', 'CountyName'], var_name='time')
    melted['time'] = pd.to_datetime(melted['time'], infer_datetime_format=True)
    melted = melted.dropna(subset=['value'])
    return melted.groupby('time').aggregate({'value':'mean'})
# %%

# %% markdown
# # Step 4: EDA and Visualization
# Perform basic EDA as you would building any model, then create at least 3 visualizations.  At least one of the visualizations should have time on the x-axis.  Axes should be labeled and any text should be legible.
# %%
# import matplotlib.pyplot as plt

# font = {'family' : 'normal',
#         'weight' : 'bold',
#         'size'   : 22}

# matplotlib.rc('font', **font)

# NOTE: if you visualizations are too cluttered to read, try calling 'plt.gcf().autofmt_xdate()'!

# to do list need to visualizate the WHole data frame to see the trens

# %%
df_5.plot(figsize = (20,10), subplots=False, legend=True)
plt.show()
# %%
df_try1.plot(figsize = (20,6), style = ".b")
import matplotlib.pyplot as plt
plt.show()
# %%
# df_yearly_mean.plot(kind = 'bar')

# %% markdown
# # Step 5: ARIMA Modeling
# Be sure to validate your model.  Make a forecast for the future and describe the trend.
# %%
# need to convert into stationary using diff
df_diff = df_yearly_mean.diff(periods = 1)
df_diff.head()
# %%
# need to drop the NA value for the 1996-12-31 year
df_diff_d = df_diff.drop(df_diff.index[0])
df_diff_d.head()
# %%
df_zip_1= df_yearly_mean.iloc[: ,10021 ]
# df_zip_2 = df_yearly_mean.iloc[ : , 10011]
# df_zip_3 = df_yearly_mean.iloc[ : ,10014]
# df_zip_4 = df_yearly_mean.iloc[ : , 94027]
# df_zip_5 = df_yearly_mean.iloc[ : ,90210]
# i wasnt to creat a function that takes ever
# %%

# %%
# Note is slowly decate is NOT stationary so we can use for an ARIMA

from statsmodels.graphics.tsaplots import plot_acf

# plot autocorrelation for each lag (alpha is confidence interval)
plot_acf(t, alpha=.05)

# %%
from statsmodels.graphics.tsaplots import plot_pacf
plot_pacf(df_5, alpha=.05, lags=20)# see if its drop
# p value is 2
# %%
from statsmodels.graphics.tsaplots import plot_pacf
plot_pacf(df_5, alpha=.05, lags=20)# see if its drop
# p value is 2
# %%
# fit model
model = ARIMA(df_5,order=(2,1,2))
model_fit = model.fit(disp=0)
print(model_fit.summary())
# %%

# %% markdown
# # Step 6: Interpreting Results
# Give a detailed interpretation of results including the relationship between the inputs and output of the model.
# %%

# %%

# %%
